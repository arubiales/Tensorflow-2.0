{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING\n",
    "#### Aquí vamos a ir viendo un poco la historia del Reinforcement Learning y los distintos métodos como han ido mejorando\n",
    "* Entorno: las características del mundo donde vivirá un agente\n",
    "* Agente: Es el encargado de vivir en el entorno y tomar **acciones** a cambio recivirá **estados y recompensas**\n",
    "* Acciones: las decisiones que toma el angente en el entorno\n",
    "* Recompensa: tras una acción el agente recive una recompensa negativa o positiva\n",
    "* Estados: la recompensa recivida cambia, el estado actual que tenía el agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La ecuación de reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La ecuación de Bellman\n",
    "* s = Estado\n",
    "* a = Acción\n",
    "* R = Recompensa\n",
    "* $\\gamma$ = Factor de descuento: sirve para que el estado actual no sea igual que el siguiente, Imaginemos que estamos en un juego, si no tenemos factor de descuento, el agente tomará decisiones aleatorias y se puede pasar una eternidad. Con el factor de descuento cada vez que toma una decisión hacemos que cada vez las recompensas sean menores, por lo que tras varios entrenamientos el agente trata de pasarse el juego lo más rápido posible para obtener una recompensa final mejor. En definitiva aceleramos el proceso de convergencia del algoritmo\n",
    "\n",
    "$$V(s) = max_a \\left(R(s,a) + \\gamma V(s') \\right) $$\n",
    "\n",
    "- Esta función es el valor que nos da el entorno en un estado concreto $V(s)$\n",
    "- El estado en el que nos vamos a encontrar después de una acción $V(s')$ \n",
    "- Hay que maximizar aquella opción que sea la mejor de todas las acciónes posibles (escoger la mejor) $max_a$\n",
    "- Dado un estado $s$, al elegir la opción $a$ la recompensa que voy a obtener en el estado actual tras tomar esa acciónb $R(s,a)$ a esto le sumarímos el valor que vamos a obtener tras moverme al nuevo estado $V(s')$, por tomar esa decisión, multiplicado por el factor de descuento $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceso de decisiónd de Markov (Markov Decision Proccess MDP)\n",
    "Hay búsquedas Determinista y No determinista. Las busquedas en el espacio de busquedas de los agentes son no deterministas, esta aleatoriedad es lo que hace que el agente pueda aprender, ya que no toma siempre la misma decisión, si no hay distintas acciones posibles a llevar a cabo con un rango de probabilidades. Esto hace que **nuestro agente no tenga en cuenta los estados anteriores, solamente tenga en cuenta el actual**. Al introducir esto en la escuación de Bellman, hace que nuestro agente no sea determinista y la ecuación quedaría así:\n",
    "$$V(s) = max_a \\left(R(s,a) + \\gamma \\sum_{s'} P(s,a,s')V(s') \\right)$$\n",
    "\n",
    "- P(s,a,s'): es la probabilidad de que están en el estado  **s, realizando la acción a**, acabemos en **s'** (el estado siguiente)\n",
    "\n",
    "**Con esto acabamos de hacer una versión de la ecuación de Bellman 2.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning (en teoría la \"Q\" proviene de calidad)\n",
    "Lo que hace es medir la calidad de la decisión tomada. Que tan buena es la decisión que hemos tomado, la Q depende de el estado en el que estaba, y en el que he acabado. Mientras que la ecuación anterior, solo dependía del estado actual (la maximización)\n",
    "$$Q(s,a) = R(s,a) + \\gamma * max_{a'} Q(s', a')$$\n",
    "\n",
    "En definitiva está función lo que hace es toma un estado, y ve el espacio de búsquedas, y para todas las decisiones posibles establece un parámetro de calidad, para saber que decisión tomar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferencia temporal\n",
    "Cuando un agente va entrenado, toma conocimiento de las acciones y sus estados. Esto lo hace porque en cada iteración, el agente toma un estado y la calidad de la decisión que tomó, y lo compara con la actual.\n",
    "$$TD(s,a) = R(s,a) + \\gamma * max_{a'} Q(s', a') - Q_{t-1}(s,a)$$\n",
    "\n",
    "Con esto podemos escribir otra formula más compleja aun\n",
    "* Alpha: es el ratio de aprendizaje por diferencia temporal\n",
    "$$Q(s,a) = Q_{t-1}(s,a) + \\alpha TD_t(a,s)$$\n",
    "\n",
    "Aquí tenemos la calidad que conocemos del estado anterior, y la calidad en el paso actual e irán actualizado sus valores en función de la diferencia temporal, multiplicado por el factor alpha. Si alpha es 1, pues lo que hacemos es aprender sin recordar lo anterior, si es 0, no aprendemos nada. **Normalmente se introducen ratio de aprendizaje bajos, 0.01 por ejemplo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "Lo que trata es de pasar el entorno en forma de matriz para suministrarselo a una red neuronal. Esto se realiza porque el Q-Learning funciona con entornos muy simples, en entornos más complejos no. Para ello es necesario aplicar Deep Q-Learning\n",
    "* El input a la red neuronal serán todos los posibles estados en los que se pueda encontrar el agente y nos va a predecir los resultados de las acciones\n",
    "* La red neuronal realiza y ajusta estas predicciones viendo el valor de los estados reales (ya que el agente ha pasado por allí y los sabe) y hacer el mismo funcionamiento que una red neuronal, calcula el error, lo propaga hacia atrás, ajusta los pesos, etc, etc.\n",
    "* Hasta aquí lo único que hacemos es entrenar los pesos\n",
    "* Tomamos una decisión y viendo los pesos y aplicandolé una función softmax para tomar la acción a llevar a cabo, quedandonos con la mejor acción posible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "No es obligatorio pero si se añade a la red neuronal, mejora los resultados de nuestro agente.  \n",
    "Por ejemplo imaginate un coche en una recta tipica de las películas. Cada vez que el coche cambie de posición, debe de introducirse nuevos inputs a la red neuronal y está producir nuevos outpus, y así constantemente. Pero en este ejemplo no tiene sentido puesto que la entrada a la red neuronal serán siempre las mismas. Esto además provoca que la red neuronal ponderé que ir en linea recta es positivo, y cuando lleguen curvas no lo hará bien, ya que los inputs habrán estado desbalanceados.  \n",
    "\n",
    "**Con el Experience Replay** lo que hacemos es que el agente guarda la información que conoce y cuando llega a un estado que ya conoce en vez de entrenar y obtener inputs, lo que hace es que tira de esta Experience Replay (de esta memoria). Lo que se hace es que se le pasa todo el input de la linea recta (en este caso) a la vez en un batch, de tal forma que aprende a realizar toda la recta de golpe y tras toda la recta se le otorga la recompensa y se actualizan los pesos  \n",
    "\n",
    "\n",
    "**Lo mismo con las curvas** en vez de ir viendo la curva posición a posición, aprende el batch entero de hacer una curva.  \n",
    "\n",
    "Se pueden almacenar tantas experiencias como quiera, de tal manera que la red neuronal antes de realizar cualquier calculo, mirará el experience Replay para ver si hay una experiencia parecida.  \n",
    "\n",
    "Esto a demás hace que **la fase de aprendizaje sea más rápido**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Políticas de selección de Acción\n",
    "Era la función softmax que vimos antes, hay tres políticas de selección de acción que podemos aplicar\n",
    "1. $\\epsilon$-greedy: en un porcenaje de veces $\\epsilon$ el agente seguirá eligiendo aleatoriamente acciones\n",
    "2. $\\epsilon$-soft: este es el contrario, en el $\\epsilon$ nos quedamos con la versión optima y el resto elegimos aleatoriamente\n",
    "3. Softmax: utiliza la función softmax de probabilidades para elegir cuales visitar en forma exploratoria de forma jerarquica según los resultados de softmax de las distintas acciones. En principio esta es mejor que los epsilon greedy porque conserva la distribución de porcentajes de la función softmax\n",
    "\n",
    "Necesitamos estás políticas porque no es suficiente con elegir la acción que maximice nuestras acciones. Aquí hay dos fases que tiene el algoritmo\n",
    "1. Exploración: detectar que hay mejores opciones para que no quearse en mínimos locales\n",
    "2. Explotación: cuando sabe que algo es bueno, va una vez y otra a ese resultado en bucle\n",
    "Con estás políticas lo que hacemos es que el agente siga aprendiendo y no se quede estancado en un mínimo local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
