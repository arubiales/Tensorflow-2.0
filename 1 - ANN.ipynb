{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales artificiales (una red neuronal normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 14s 3us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redimensionar el dataset para aplanarlo\n",
    "Como vamos a utilizar una red neuronal simple, totalmente conectada, transformamos las matrices a formato vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de la Red Neuronal Artificial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una primera capa totalmente conectada que constará:\n",
    "1. Capa densa de 128 unidades cuyos parametros de entrada son 784 filas al aplanar 28*28 son 784\n",
    "2. Un dropout del 20% lo que hará qué el 20% de las neuronas no se actualicen y así evitar overfitting\n",
    "3. añadimos una capa de salida densa con las 10 unidades, una por cada clase\n",
    "4. Compilamos diciento el optimizador, la función de perdidas y la métrica que mide lo bueno que va siendo el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 302,474\n",
      "Trainable params: 301,706\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=256, activation='relu', input_shape=(784, )))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(units=256, activation='relu', input_shape=(784, )))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784, )))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 10s 159us/sample - loss: 0.6673 - sparse_categorical_accuracy: 0.7588\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.5258 - sparse_categorical_accuracy: 0.8109\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 9s 156us/sample - loss: 0.4854 - sparse_categorical_accuracy: 0.8243\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 10s 167us/sample - loss: 0.4584 - sparse_categorical_accuracy: 0.8350\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 10s 167us/sample - loss: 0.4432 - sparse_categorical_accuracy: 0.8392\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 10s 163us/sample - loss: 0.4311 - sparse_categorical_accuracy: 0.8440\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 10s 168us/sample - loss: 0.4222 - sparse_categorical_accuracy: 0.8459\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 10s 163us/sample - loss: 0.4104 - sparse_categorical_accuracy: 0.8495\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s 162us/sample - loss: 0.4051 - sparse_categorical_accuracy: 0.8523\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 10s 164us/sample - loss: 0.3947 - sparse_categorical_accuracy: 0.8568\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 10s 166us/sample - loss: 0.3916 - sparse_categorical_accuracy: 0.8585\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 10s 160us/sample - loss: 0.3845 - sparse_categorical_accuracy: 0.8593\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 10s 161us/sample - loss: 0.3768 - sparse_categorical_accuracy: 0.8622\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 10s 170us/sample - loss: 0.3708 - sparse_categorical_accuracy: 0.8635\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.3720 - sparse_categorical_accuracy: 0.8637\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.3669 - sparse_categorical_accuracy: 0.8649\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 12s 205us/sample - loss: 0.3633 - sparse_categorical_accuracy: 0.8669\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 11s 180us/sample - loss: 0.3623 - sparse_categorical_accuracy: 0.8676\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 10s 175us/sample - loss: 0.3584 - sparse_categorical_accuracy: 0.8692\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 10s 172us/sample - loss: 0.3544 - sparse_categorical_accuracy: 0.8691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22aa5868848>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 63us/sample - loss: 0.3671 - sparse_categorical_accuracy: 0.8690\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
